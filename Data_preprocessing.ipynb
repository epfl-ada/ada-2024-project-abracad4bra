{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean CMU dataset and filter horror movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU_headers = ['Weekipedia_ID', 'Freebase_ID', 'Name', 'Release_date', 'Revenue', 'Runtime', 'Language', 'Countries', 'Genres']\n",
    "CMU = pd.read_csv('MovieSummaries/movie.metadata.tsv', sep='\\t', names=CMU_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU['Release_year'] = CMU['Release_date'].astype(str).str.extract(r'(\\d{4})')\n",
    "CMU['Release_year'] = pd.to_numeric(CMU['Release_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(column):\n",
    "    return column.apply(ast.literal_eval).apply(lambda x: list(x.values()))\n",
    "\n",
    "cleaning_columns = ['Language', 'Genres', 'Countries']\n",
    "\n",
    "for column_name in cleaning_columns:\n",
    "    CMU[column_name+\"_clean\"] = extract_words(CMU[column_name])\n",
    "\n",
    "def clean_language(language_string):\n",
    "    return re.sub(\" Language\", \"\", language_string)\n",
    "\n",
    "CMU.Language_clean = CMU.Language_clean.apply(lambda lang_list: list(map(clean_language, lang_list)))\n",
    "\n",
    "isHorrorMovie = CMU['Genres_clean'].apply(lambda l: 'Horror' in l)\n",
    "horror_df = CMU[isHorrorMovie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU = CMU.drop(['Release_date', 'Language', 'Countries', 'Genres'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMU_horror_df = CMU[CMU['Genres_clean'].apply(lambda l: 'Horror' in l)]\n",
    "\n",
    "CMU_horror_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean additional horror movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_df = pd.read_csv('horror_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32540, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_df['Release_year'] = horror_df['release_date'].astype(str).str.extract(r'(\\d{4})')\n",
    "horror_df['Release_year'] = pd.to_numeric(horror_df['Release_year'])\n",
    "horror_df = horror_df.drop(['original_title', 'poster_path', 'status', 'adult', 'backdrop_path', 'collection', 'release_date'], axis=1)\n",
    "\n",
    "horror_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_df['genre_names'] = horror_df['genre_names'].str.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the two datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horror_movies = pd.merge(CMU_horror_df, horror_df, left_on='Name',right_on='title', how='outer')\n",
    "Horror_movies = pd.merge(\n",
    "    CMU_horror_df, \n",
    "    horror_df, \n",
    "    left_on=['Name', 'Release_year'], \n",
    "    right_on=['title', 'Release_year'], \n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horror_movies['Name'] = Horror_movies['Name'].combine_first(Horror_movies['title'])\n",
    "Horror_movies['Runtime'] = Horror_movies['Runtime'].combine_first(Horror_movies['runtime'])\n",
    "Horror_movies['Revenue'] = Horror_movies['Revenue'].combine_first(Horror_movies['revenue'])\n",
    "Horror_movies['Genres'] = Horror_movies['Genres_clean'].combine_first(Horror_movies['genre_names'])\n",
    "\n",
    "Horror_movies = Horror_movies.drop(['title', 'runtime', 'revenue', 'Genres_clean', 'genre_names'], axis=1)\n",
    "Horror_movies['Name'] = Horror_movies['Name'].str.replace(r'^[!#]+', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horror_movies['ID'] = Horror_movies.index + 1 \n",
    "Horror_movies = Horror_movies.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a text file with all summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMU_plot_summaries = pd.read_csv('HorrorMovieSummaries.txt', sep='\\t', header=None, names=['Weekipedia_ID', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = Horror_movies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that both 'wiki_id' columns are of the same type\n",
    "CMU_plot_summaries['Weekipedia_ID'] = CMU_plot_summaries['Weekipedia_ID'].astype(str)\n",
    "merged_df['Weekipedia_ID'] = merged_df['Weekipedia_ID'].astype(str)\n",
    "\n",
    "# Merge on 'wiki_id' to get the summaries in merged_df\n",
    "merged_df = pd.merge(merged_df, CMU_plot_summaries, on='Weekipedia_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Summary'] = merged_df['overview'].combine_first(merged_df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_file = 'Summaries.txt'\n",
    "\n",
    "# Write summaries to the output file\n",
    "with open(output_file, 'w') as f:\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Write each line in the format: new ID followed by the summary\n",
    "        f.write(f\"{row['ID']}\\t{row['Summary']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_file = 'Taglines.txt'\n",
    "\n",
    "# Write taglines to the output file, checking for non-NaN values\n",
    "with open(output_file, 'w') as f:\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Check if 'tagline' is not NaN\n",
    "        if pd.notna(row['tagline']):\n",
    "            # Write each line in the format: new ID followed by the tagline\n",
    "            f.write(f\"{row['ID']}\\t{row['tagline']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horror_movies = Horror_movies.drop(['overview', 'tagline'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horror_movies.to_csv('Horror_Movies_Clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_columns = [\n",
    "   'Wikipedia movie ID', 'Freebase Movie ID', 'Movie release date','Character Name', 'Actor DOB', 'Actor gender', 'Actor height', 'Actor ethnicity', \n",
    "    'Actor Name', 'Actor age at movie release', 'Freebase character map1', 'Freebase character map2', 'Freebase character map3'\n",
    "]\n",
    "character_metadata = pd.read_csv('MovieSummaries/character.metadata.tsv', sep='\\t', names = character_columns)\n",
    "\n",
    "actors_and_movies = pd.merge(Horror_movies, character_metadata, left_on='Weekipedia_ID', right_on='Wikipedia movie ID', how='left')\n",
    "actors_and_movies['Name'] = actors_and_movies['Name'].str.replace(r'^[!#]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekipedia_ID</th>\n",
       "      <th>Freebase_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Release_year</th>\n",
       "      <th>Language_clean</th>\n",
       "      <th>Countries_clean</th>\n",
       "      <th>original_language</th>\n",
       "      <th>popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>Character Name</th>\n",
       "      <th>Actor DOB</th>\n",
       "      <th>Actor gender</th>\n",
       "      <th>Actor height</th>\n",
       "      <th>Actor ethnicity</th>\n",
       "      <th>Actor Name</th>\n",
       "      <th>Actor age at movie release</th>\n",
       "      <th>Freebase character map1</th>\n",
       "      <th>Freebase character map2</th>\n",
       "      <th>Freebase character map3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1915House</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.600</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alive</td>\n",
       "      <td>13416285.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>50.907</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue_Whale</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>0.840</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Captured</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>4.197</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EATPRETTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.600</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weekipedia_ID Freebase_ID        Name     Revenue  Runtime  Release_year  \\\n",
       "0            NaN         NaN   1915House         0.0     55.0        2018.0   \n",
       "1            NaN         NaN       Alive  13416285.0     98.0        2020.0   \n",
       "2            NaN         NaN  Blue_Whale         0.0     93.0        2021.0   \n",
       "3            NaN         NaN    Captured         0.0     81.0        2017.0   \n",
       "4            NaN         NaN   EATPRETTY         0.0      4.0        2018.0   \n",
       "\n",
       "  Language_clean Countries_clean original_language  popularity  ...  \\\n",
       "0            NaN             NaN                en       0.600  ...   \n",
       "1            NaN             NaN                ko      50.907  ...   \n",
       "2            NaN             NaN                ru       0.840  ...   \n",
       "3            NaN             NaN                en       4.197  ...   \n",
       "4            NaN             NaN                en       0.600  ...   \n",
       "\n",
       "   Character Name  Actor DOB  Actor gender Actor height Actor ethnicity  \\\n",
       "0             NaN        NaN           NaN          NaN             NaN   \n",
       "1             NaN        NaN           NaN          NaN             NaN   \n",
       "2             NaN        NaN           NaN          NaN             NaN   \n",
       "3             NaN        NaN           NaN          NaN             NaN   \n",
       "4             NaN        NaN           NaN          NaN             NaN   \n",
       "\n",
       "   Actor Name  Actor age at movie release Freebase character map1  \\\n",
       "0         NaN                         NaN                     NaN   \n",
       "1         NaN                         NaN                     NaN   \n",
       "2         NaN                         NaN                     NaN   \n",
       "3         NaN                         NaN                     NaN   \n",
       "4         NaN                         NaN                     NaN   \n",
       "\n",
       "  Freebase character map2 Freebase character map3  \n",
       "0                     NaN                     NaN  \n",
       "1                     NaN                     NaN  \n",
       "2                     NaN                     NaN  \n",
       "3                     NaN                     NaN  \n",
       "4                     NaN                     NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actors_and_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnicity_dict={\n",
    "    \"/m/01kb9y\": \"Multiracial\",\n",
    "    \"/m/05qb937\": \"Venezuelans\",\n",
    "    \"/m/09v5bdn\": \"Puerto Ricans\",\n",
    "    \"/m/02pfy17\": \"Syrian people\",\n",
    "    \"/m/013xrm\": \"Germans\",\n",
    "    \"/m/01n94b\": \"Slovaks\",\n",
    "    \"/m/02w7gg\": \"English people\",\n",
    "    \"/m/0x67\": \"African American\",\n",
    "    \"/m/011bn6ys\": None,\n",
    "    \"/m/0118b8ry\": None,\n",
    "    \"/m/03bkbh\": \"Irish people\",\n",
    "    \"/m/0318mh\": \"Finns\",\n",
    "    \"/m/027hhf\": \"Arbëreshë people\",\n",
    "    \"/m/04c28\": \"Kurds\",\n",
    "    \"/m/0cx3p\": \"Berbers\",\n",
    "    \"/m/032j30\": \"Native Hawaiians\",\n",
    "    \"/m/0gcp7x\": \"Iranian Azerbaijanis\",\n",
    "    \"/m/013xrm\": \"Germans\",\n",
    "    \"/m/038723\": \"Greek American\",\n",
    "    \"/m/0d2by\": \"Chinese American\",\n",
    "    \"/m/09vc4s\": \"English American\",\n",
    "    \"/m/0912ll\": \"Dominican American\",\n",
    "    \"/m/07mqps\": \"Dutch-American\",\n",
    "    \"/m/01qhm_\": \"German American\",\n",
    "    \"/m/0dbxy\": \"Cherokee\",\n",
    "    \"/m/013s41\": \"Bulgarians\",\n",
    "    \"/m/01km_m\": \"Slovenes\",\n",
    "    \"/m/02ctzb\": \"White people\",\n",
    "    \"/m/033tf_\": \"Irish American\",\n",
    "    \"/m/0222qb\": \"Italian people\",\n",
    "    \"/m/0jt85pd\": \"Greeks\",\n",
    "    \"/m/03w9xlf\": \"Filipino Italian\",\n",
    "    \"/m/0j251_s\": \"Arabs in France\",\n",
    "    \"/m/0bwhd5z\": \"Harari people\",\n",
    "    \"/m/0k0t_dz\": \"Caucasian race\",\n",
    "    \"/m/09743\": \"Pashtun\",\n",
    "    \"/m/03lmx1\": \"Scottish people\",\n",
    "    \"/m/0bpjh3\": \"Bengalis\",\n",
    "    \"/m/0j63_pr\": \"French Canadian American\",\n",
    "    \"/m/0jt8h6f\": \"Latin Americans\",\n",
    "    \"/m/02gx2x\": \"Javanese people\",\n",
    "    \"/m/048z7l\": \"Jewish American\",\n",
    "    \"/m/03ts0c\": \"French people\",\n",
    "    \"/m/013s3n\": \"Czechs\",\n",
    "    \"/m/0268_k\": \"Danes\",\n",
    "    \"/m/059_w\": \"Native Americans in the United States\",\n",
    "    \"/m/09kr66\": \"Russian American\",\n",
    "    \"/m/0f3v0\": \"Comanche\",\n",
    "    \"/m/09743\": \"Pashtun\",\n",
    "    \"/m/09vc4s\": \"English American\",\n",
    "    \"/m/0bpjh3\": \"Bengalis\",\n",
    "    \"/m/0x67\": \"African American\",\n",
    "    \"/m/0j3c70b\": \"Jamaicans\",\n",
    "    \"/m/0dryh9k\": \"Indian people\",\n",
    "    \"/m/09vc4s\": \"English American\",\n",
    "    \"/m/0dbxy\": \"Cherokee\",\n",
    "    \"/m/03bkbh\": \"Irish people\",\n",
    "    \"/m/02pfy17\": \"Syrian people\",\n",
    "    \"/m/0cx3p\": \"Berbers\",\n",
    "    \"/m/03bkbh\": \"Irish people\",\n",
    "    \"/m/02ctzb\": \"White people\",\n",
    "    \"/m/02w7gg\": \"English people\",\n",
    "    \"/m/033tf_\": \"Irish American\",\n",
    "    \"/m/09vc4s\": \"English American\",\n",
    "    \"/m/0bwhd5z\": \"Harari people\",\n",
    "    \"/m/0k0t_dz\": \"Caucasian race\",\n",
    "    \"/m/013xrm\": \"Germans\",\n",
    "    \"/m/09743\": \"Pashtun\",\n",
    "    \"/m/03w9xlf\": \"Filipino Italian\",\n",
    "    \"/m/0j251_s\": \"Arabs in France\",\n",
    "    \"/m/04c28\": \"Kurds\",\n",
    "    \"/m/0j63_pr\": \"French Canadian American\",\n",
    "    \"/m/038723\": \"Greek American\",\n",
    "    \"/m/0134vqyy\": \"Scottish American\",\n",
    "    \"/m/0dbxy\": \"Cherokee\",\n",
    "    \"/m/05qb937\": \"Venezuelans\",\n",
    "    \"/m/033tf_\": \"Irish American\",\n",
    "    \"/m/03bkbh\": \"Irish people\",\n",
    "    \"/m/04c28\": \"Kurds\",\n",
    "    \"/m/0j251_s\": \"Arabs in France\",\n",
    "    \"/m/0bpjh3\": \"Bengalis\",\n",
    "    \"/m/0dbxy\": \"Cherokee\",\n",
    "    \"/m/038723\": \"Greek American\"\n",
    "}\n",
    "\n",
    "actors_and_movies['Ethnicity'] = actors_and_movies['Actor ethnicity'].map(ethnicity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
