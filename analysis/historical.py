import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import os


terrorism_keywords = [
    # List of keywords related to terrorism, generated by chat-GPT
    "terrorism", "terrorist", "terror", "extremism", "extremist", 
    "radicalism", "radical", "bombing", "explosion", "hijacking", 
    "hostage", "violence", "insurgency", "militant", "jihad", 
    "jihadist", "suicide", "martyr", "detonate", "detonation", 
    "attack", "assassination", "threat", "plot", "cell", 
    "sabotage", "warfare", "guerrilla", "intimidation", 
    "fear", "panic", "fundamentalism", "al-Qaeda", "ISIS", 
    "radicalization", "extremism", "counterterrorism", "militia", 
    "revolutionary", "sectarian", "violence", "caliphate"
]
terrorism_events = {
    # Define the dictionary of historical events
    1972: "Munich Olympics Massacre",
    1988: "Pan Am Flight 103 Lockerbie \n Bombing",
    1993: "World Trade Center Bombing",
    2001: "9/11",
    2004: "Madrid Train Bombings",
    2015: "Paris Attacks",
    2019: "Christchurch Shootings"
}


flu_keywords = [
    # List of keywords related to flu, generated by chat-GPT
    "flu", "influenza", "virus", "pandemic", "epidemic", "outbreak", 
    "fever", "cough", "sneeze", "contagion", "contagious", "vaccine", 
    "vaccination", "quarantine", "isolation", "immunity", "symptom", 
    "pathogen", "infection", "infectious", "hygiene", "sanitation", 
    "pandemics", "seasonal", "transmission", "mutation", "swine", 
    "avian", "respiratory", "illness", "treatment", "medication", 
    "public", "health", "prevention", "spread", "diagnosis", "therapy", 
    "pharmaceutical", "recovery"
]
flu_events = {
    # Flu-related events
    1968: "Hong Kong Flu Epidemic",
    1977: "Russian Flu Pandemic",
    1997: "Avian Flu Outbreak in Hong Kong",
    2003: "SARS Epidemic",
    2009: "Swine Flu Pandemic",
    2013: "Avian Influenza Outbreak",
    2020: "COVID-19 Pandemic"
}


petrol_keywords = [
    # List of keywords related to petrol crisis, generated by chat-GPT
    "oil", "fuel", "energy", "gasoline", "crude", "refinery", 
    "embargo", "scarcity", "supply", "demand", "prices", "OPEC", 
    "market", "blackout", "shortage", "pipeline", "tar", "reserve", 
    "drilling", "exploration", "geopolitics", "cartel", "inflation", 
    "stockpiles", "consumption", "power", "transportation", "diesel", 
    "gas", "alternative", "subsidy", "export", "import", "global", "economy", 
    "competition", "recession", "climate", "carbon", "sustainability", 
    "tax", "fossil", "crisis"
]
petrol_events = {
    # Significant petrol-related events
    1973: "1st Oil Crisis",
    1979: "2nd Oil Crisis",
    1990: "Gulf War",
    2008: "Global Financial Crisis",
    2014: "Oil Price Crash",
    2020: "COVID-19 \n Pandemic Oil Impact"
}

communism_keywords = [
    # List of keywords related to communism, generated by chat-GPT
    "communism", "equality", "revolution", "proletariat", "collectivism", 
    "socialism", "Marxism", "class", "bourgeoisie", "labor", "solidarity", 
    "state", "redistribution", "ideology", "party", "collective", "worker", "union", 
    "comrade", "land", "people", "reform", "ownership", "struggle", "equality", 
    "freedom", "manifesto", "nation", "policy", "community", "control", "authority", 
    "dictatorship", "plan", "economy", "utopia", "power", "rights", "progress", 
    "system", "justice", "common"
]
communism_events = {
    # Historical communism events determined with chat-GPT
    1961: "Construction of the\n Berlin Wall",
    1966: "Chinese Cultural\nRevolution",
    1989: "Fall of the Berlin Wall",
    1991: "Dissolution of the\nSoviet Union",
    2016: "Cuban-American Thaw"
}

nuclear_keywords = [
    # List of keywords related to nuclear, generated by chat-GPT
    "nuclear", "radiation", "disaster", "meltdown", "reactor", "Chernobyl", 
    "Fukushima", "atomic", "fallout", "radioactive", "contamination", 
    "nucleon", "plume", "nucleus", "radiological", "uranium", "plutonium", 
    "exposure", "radiation", "hazard", "energy", "core", "criticality", "waste", 
    "mutation", "reactivity", "isotope", "fusion", "bomb", "energy", "heat", 
    "spill", "hazardous", "radiotherapy", "pollution", "emergency", "radiation", 
    "safety", "risk", "breach", "accident", "fallout", "toxic"                  
]
nuclear_events = {
    # Historical nuclear events determined with Chat-GPT
    1979: "Three Mile Island\nNuclear Accident",
    1986: "Chernobyl Nuclear \nDisaster",
    1991: "Cessation\nof Soviet Nuclear Testing",
    1999: "Tokaimura Nuclear\nAccident",
    2011: "Fukushima Daiichi\nNuclear Disaster",
    2019: "North Korean Nuclear\nWeapons Testing Escalation",
}

# Usefull functions
# The following two functions are used to evaluate the similarity between a subject (represented by some keywords) and the summaries form the data frame. 
def get_average_vector(keywords, model):
    # Calculates the average vector for a list of keywords, according to the model previously trained
    vectors = [model.get_word_vector(word) for word in keywords if word in model] # extracts the vector of each word of the list, from the model
    if vectors:
        return np.mean(vectors, axis=0) # return vector with mean of all vectors
    else:
        return np.zeros(model.get_dimension())

def calculate_similarity(summary, subject_vector, model):
    # Calculates a cosine similarity score between a summary and a vector
    tokens = summary.split() # slip the string into a list of substrings
    summary_vectors = [model.get_word_vector(word) for word in tokens if word in model] # extracts the vector of each word of the list, fromm the model
    if not summary_vectors:
        return 0
    summary_vector = np.mean(summary_vectors, axis=0)
    return cosine_similarity([summary_vector], [subject_vector])[0][0] 

def merge_files(output_file, part_prefix):
    # Function that merges binary files
    # Sorting sub files
    parts = sorted([f for f in os.listdir() if f.startswith(part_prefix)])
    
    # Reconstructing original file
    with open(output_file, "wb") as outfile:
        for part in parts:
            with open(part, "rb") as infile:
                outfile.write(infile.read())

def analyze_similarity(df, similarity_col, events, threshold):
    # Function that plots the evolution of the similarity between a subject's lexical field 
    # and horror movies summaries. It also statistically evaluates the significance for historical events

    # Create a binary column for the presence/absence of the subject according to threshold
    df[f'Contains_{similarity_col}'] = df[similarity_col].apply(lambda x: 1 if x > threshold else 0)
    presence_subject = df.groupby(df["Release_year"])[[f'Contains_{similarity_col}']].sum()
    
    # Total number of horror movies per year
    total_movies_per_year = df.groupby(df["Release_year"])['Summary'].count()
    
    # Proportion of horror movies which summary is influenced by the subject's lexical fields per year
    proportion_subject = presence_subject[f'Contains_{similarity_col}'] / total_movies_per_year

    years_of_interest = list(events.keys())
    results = {}

    # Loop over each year of interest
    for year in years_of_interest:
        
        # Range of years before and after the year of interest
        before_range = [year-1, year, year + 1]
        after_range = [year + 1, year + 2, year + 3, year + 4, year + 5]

        # Filter dataframe
        before_counts = df[df['Release_year'].dt.year.isin(before_range)].groupby(df['Release_year'].dt.year)[f'Contains_{similarity_col}'].sum()
        after_counts = df[df['Release_year'].dt.year.isin(after_range)].groupby(df['Release_year'].dt.year)[f'Contains_{similarity_col}'].sum()

        # Years of minimum and maximum occurrences in range
        min_year = before_counts.idxmin() if not before_counts.empty else None
        max_year = after_counts.idxmax() if not after_counts.empty else None

        # Make sure max_year is not smaller than min_year to avoid decrease in number of movies
        if min_year >= max_year:
            max_year += 1
            if min_year == max_year:
                max_year += 1
                
        if min_year is not None and max_year is not None:
            # Filter dataframe for the adequate min and max year
            min_year_data = df[df['Release_year'].dt.year == min_year][f'Contains_{similarity_col}']
            max_year_data = df[df['Release_year'].dt.year == max_year][f'Contains_{similarity_col}']

            if len(min_year_data) > 0 and len(max_year_data) > 0:
                # Mann-Whitney U test
                stat, p_value = mannwhitneyu(min_year_data, max_year_data)
                
                # Assign result values
                results[year] = {
                    "statistic": stat,
                    "p_value": p_value,
                    "min_year": min_year,
                    "max_year": max_year,
                    "min_year_count": before_counts[min_year],
                    "max_year_count": after_counts[max_year]
                }
               
    # Transform into dataframe
    df_stats = pd.DataFrame(results).T
    
    # Filter significant events
    df_stats['Significant'] = df_stats['p_value'] < 0.05


    ### Plot
    plt.style.use('dark_background')
    fig, ax1 = plt.subplots(figsize=(12, 7))

    # Title with custom text formatting
    ax1.set_title(f"Evolution of the similarity between lexical field of {similarity_col.split('_')[0].lower()} and horror movies summaries (Threshold: {threshold:.2f})")

    # Total horror movie
    ax1.plot(total_movies_per_year.index, total_movies_per_year.values, color='#ff7100', label='Total horror movies per year')
    ax1.set_xlabel("Year")
    ax1.set_ylabel("Count", color='#ff7100')
    ax1.tick_params(axis='y', labelcolor='#ff7100')
    ax1.legend(loc='upper left')
    ax1.set_ylim(0, total_movies_per_year.max() * 1.6)

    # Proportion of movies which summaries are influenced by subject lexical field
    ax2 = ax1.twinx()
    ax2.spines['right'].set_position(('axes', 1.0))
    ax2.plot(proportion_subject.index, proportion_subject * 100, color='#ae03ff', linestyle='-', label=f'{similarity_col.split("_")[0].title()} influenced horror movies summary')
    ax2.set_ylabel("Proportion (%)", color='#ae03ff')
    ax2.tick_params(axis='y', labelcolor='#ae03ff')
    ax2.legend(loc='upper right')
    ax2.set_ylim(0, proportion_subject.max() * 100 * 2)

    # Labels of events 
    last_plotted_year = None
    y_positions = [total_movies_per_year.max() * 1.1, total_movies_per_year.max() * 1] 

    for year, event in events.items():
        p_value = df_stats.loc[year, "p_value"] if year in df_stats.index else None
        if p_value is not None:

            # Condition on color
            if p_value < 0.05:
                color = '#008000'
            elif 0.05 <= p_value < 0.10:
                color = '#f1c232'
            else:
                color = '#ff007f'
                
            label_text = f'{event}\n {year} (p={p_value:.3f})'

            # Condition to avoid overlapping the text
            if last_plotted_year and abs(year - last_plotted_year) < 5:
                y_position = y_positions[1]
            else:
                y_position = y_positions[0]

            # Vertical line
            ax1.axvline(x=pd.to_datetime(str(year)), color='grey', linestyle='--', linewidth=1)
            
            # Text
            ax1.text(
                pd.to_datetime(str(year)), 
                y_position, 
                label_text,
                color=color, fontsize=10, ha='center', va='bottom', rotation=45
            )
            
            last_plotted_year = year
            
    plt.tight_layout()
    plt.show()




